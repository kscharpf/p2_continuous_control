{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "## 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "skipRandom = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter and Architecture Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import *\n",
    "def train_ddpg(batch_size, buffer_size, use_batch_norm, random_fn, num_episodes):\n",
    "    agent = Agent(numAgents = num_agents, \n",
    "                  state_size=state_size, \n",
    "                  action_size=action_size, \n",
    "                  random_seed=47,\n",
    "                  batch_size = batch_size,\n",
    "                  buffer_size = buffer_size,\n",
    "                  use_batch_norm = use_batch_norm,\n",
    "                  random_fn = random_fn)\n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "    PRINT_EVERY=1\n",
    "    for episode in range(num_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        agent.reset()\n",
    "\n",
    "        score = np.zeros(num_agents)\n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            score += np.array(rewards)\n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        if episode % PRINT_EVERY == 0:\n",
    "            print(\"Episode {} Min/Avg/Max {:.4E}/{:.4E}/{:.4E}\".format(\n",
    "                episode, np.min(score), np.mean(score), np.max(score)))\n",
    "        scores.append(np.mean(score))\n",
    "    return scores, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Hyperparameter and Model Architecture Search\n",
    "Here, we evaluate the performance over 50 episodes of three different batch sizes [64,128,256], two different buffer sizes [1e5,1e6], and two different network architectures (with and without a batch normalization layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "skipThis = True\n",
    "if not skipThis:\n",
    "    for batch_size in [64,128,256]:\n",
    "        for buffer_size in [100000,1000000]:\n",
    "            for use_batch_norm in [False,True]:\n",
    "                for random_fn, fn_name in [(np.random.randn, 'normal')]:\n",
    "                    start_time = time.time()\n",
    "                    scores, agent = train_ddpg(batch_size,\n",
    "                                       buffer_size,\n",
    "                                       use_batch_norm,\n",
    "                                       random_fn,\n",
    "                                       50)\n",
    "                    end_time = time.time()\n",
    "                    fname_suffix = '{}_{}_{}_{}'.format(batch_size, buffer_size, use_batch_norm, fn_name)\n",
    "                    fname = './data/time_and_scores_{}.pkl'.format(fname_suffix)\n",
    "                    pickle.dump((end_time-start_time, scores), open(fname, 'wb'))\n",
    "                    torch.save(agent.actor_local.state_dict(), './data/actor_{}.pt'.format(fname_suffix))\n",
    "                    torch.save(agent.critic_local.state_dict(), './data/critic_{}.pt'.format(fname_suffix))\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train the Full Model\n",
    "Train a model over 300 episodes using the mini-batch size of 128, the buffer size of 1e6, and using the batch normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Min/Avg/Max 0.0000E+00/6.5500E-02/5.4000E-01\n",
      "Episode 1 Min/Avg/Max 3.0000E-02/5.6000E-01/1.3400E+00\n",
      "Episode 2 Min/Avg/Max 2.0000E-02/7.4800E-01/1.9400E+00\n",
      "Episode 3 Min/Avg/Max 7.0000E-02/9.9550E-01/2.3000E+00\n",
      "Episode 4 Min/Avg/Max 2.9000E-01/8.6150E-01/2.3300E+00\n",
      "Episode 5 Min/Avg/Max 1.0000E-01/7.4800E-01/1.3200E+00\n",
      "Episode 6 Min/Avg/Max 9.0000E-02/7.2150E-01/1.7800E+00\n",
      "Episode 7 Min/Avg/Max 2.0000E-01/6.7050E-01/1.3300E+00\n",
      "Episode 8 Min/Avg/Max 1.9000E-01/8.1100E-01/2.1400E+00\n",
      "Episode 9 Min/Avg/Max 0.0000E+00/8.0200E-01/1.4100E+00\n",
      "Episode 10 Min/Avg/Max 3.0000E-01/1.1950E+00/2.6500E+00\n",
      "Episode 11 Min/Avg/Max 0.0000E+00/1.3235E+00/3.5500E+00\n",
      "Episode 12 Min/Avg/Max 4.1000E-01/1.1895E+00/2.5400E+00\n",
      "Episode 13 Min/Avg/Max 5.5000E-01/1.5605E+00/2.2200E+00\n",
      "Episode 14 Min/Avg/Max 8.4000E-01/1.7370E+00/3.0100E+00\n",
      "Episode 15 Min/Avg/Max 3.1000E-01/1.1680E+00/1.9600E+00\n",
      "Episode 16 Min/Avg/Max 6.0000E-02/1.2890E+00/2.3800E+00\n",
      "Episode 17 Min/Avg/Max 5.1000E-01/1.5280E+00/4.0500E+00\n",
      "Episode 18 Min/Avg/Max 5.4000E-01/1.9110E+00/3.5500E+00\n",
      "Episode 19 Min/Avg/Max 2.7000E-01/1.8210E+00/3.7200E+00\n",
      "Episode 20 Min/Avg/Max 4.9000E-01/3.3980E+00/6.9300E+00\n",
      "Episode 21 Min/Avg/Max 8.4000E-01/2.8140E+00/7.0900E+00\n",
      "Episode 22 Min/Avg/Max 1.5700E+00/3.6720E+00/5.8100E+00\n",
      "Episode 23 Min/Avg/Max 1.6300E+00/3.3890E+00/5.6700E+00\n",
      "Episode 24 Min/Avg/Max 1.2600E+00/3.6340E+00/7.6900E+00\n",
      "Episode 25 Min/Avg/Max 1.9300E+00/4.2905E+00/9.1200E+00\n",
      "Episode 26 Min/Avg/Max 1.7800E+00/5.3840E+00/1.4230E+01\n",
      "Episode 27 Min/Avg/Max 2.2500E+00/6.1270E+00/1.2490E+01\n",
      "Episode 28 Min/Avg/Max 3.8600E+00/5.6780E+00/8.1300E+00\n",
      "Episode 29 Min/Avg/Max 2.5100E+00/5.0430E+00/7.0800E+00\n",
      "Episode 30 Min/Avg/Max 3.4500E+00/6.6015E+00/1.6620E+01\n",
      "Episode 31 Min/Avg/Max 4.7100E+00/7.4565E+00/1.5150E+01\n",
      "Episode 32 Min/Avg/Max 5.1400E+00/8.4315E+00/1.8440E+01\n",
      "Episode 33 Min/Avg/Max 4.8300E+00/7.6935E+00/1.2290E+01\n",
      "Episode 34 Min/Avg/Max 5.3400E+00/8.0090E+00/1.4720E+01\n",
      "Episode 35 Min/Avg/Max 4.5300E+00/7.3940E+00/1.1160E+01\n",
      "Episode 36 Min/Avg/Max 5.6000E+00/9.6295E+00/1.5590E+01\n",
      "Episode 37 Min/Avg/Max 5.8000E+00/1.0236E+01/2.0230E+01\n",
      "Episode 38 Min/Avg/Max 5.0000E+00/9.5960E+00/1.2570E+01\n",
      "Episode 39 Min/Avg/Max 3.6000E+00/9.7800E+00/1.6690E+01\n",
      "Episode 40 Min/Avg/Max 3.1700E+00/8.8205E+00/1.3990E+01\n",
      "Episode 41 Min/Avg/Max 1.2700E+00/1.0592E+01/1.6840E+01\n",
      "Episode 42 Min/Avg/Max 5.9900E+00/1.1114E+01/2.5320E+01\n",
      "Episode 43 Min/Avg/Max 4.6100E+00/1.2212E+01/3.8060E+01\n",
      "Episode 44 Min/Avg/Max 1.4000E-01/1.2261E+01/2.2670E+01\n",
      "Episode 45 Min/Avg/Max 9.4100E+00/1.4146E+01/1.8850E+01\n",
      "Episode 46 Min/Avg/Max 7.0000E-01/1.2886E+01/1.6630E+01\n",
      "Episode 47 Min/Avg/Max 9.9000E-01/1.4029E+01/1.8960E+01\n",
      "Episode 48 Min/Avg/Max 1.0600E+01/1.6672E+01/2.0210E+01\n",
      "Episode 49 Min/Avg/Max 9.4700E+00/1.6301E+01/2.8620E+01\n",
      "Episode 50 Min/Avg/Max 6.5700E+00/1.6574E+01/2.2240E+01\n",
      "Episode 51 Min/Avg/Max 1.4450E+01/1.7871E+01/3.2600E+01\n",
      "Episode 52 Min/Avg/Max 1.2140E+01/1.6562E+01/2.1280E+01\n",
      "Episode 53 Min/Avg/Max 0.0000E+00/1.6682E+01/3.4200E+01\n",
      "Episode 54 Min/Avg/Max 1.0310E+01/1.6612E+01/2.5180E+01\n",
      "Episode 55 Min/Avg/Max 1.1930E+01/1.7693E+01/2.1990E+01\n",
      "Episode 56 Min/Avg/Max 1.2970E+01/1.8883E+01/2.3600E+01\n",
      "Episode 57 Min/Avg/Max 1.0750E+01/1.8660E+01/2.2770E+01\n",
      "Episode 58 Min/Avg/Max 1.5500E+00/1.8468E+01/2.3910E+01\n",
      "Episode 59 Min/Avg/Max 6.5600E+00/1.8334E+01/2.4580E+01\n",
      "Episode 60 Min/Avg/Max 1.3070E+01/2.1296E+01/3.7830E+01\n",
      "Episode 61 Min/Avg/Max 1.0670E+01/1.9762E+01/2.5010E+01\n",
      "Episode 62 Min/Avg/Max 1.7000E-01/2.1406E+01/3.9050E+01\n",
      "Episode 63 Min/Avg/Max 1.5410E+01/2.0757E+01/2.5380E+01\n",
      "Episode 64 Min/Avg/Max 1.8000E+00/1.9743E+01/2.5760E+01\n",
      "Episode 65 Min/Avg/Max 1.5740E+01/2.0802E+01/2.6050E+01\n",
      "Episode 66 Min/Avg/Max 1.6690E+01/2.3203E+01/2.8230E+01\n",
      "Episode 67 Min/Avg/Max 1.7270E+01/2.3473E+01/2.9840E+01\n",
      "Episode 68 Min/Avg/Max 1.6600E+01/2.2996E+01/3.1610E+01\n",
      "Episode 69 Min/Avg/Max 1.8280E+01/2.3678E+01/3.5310E+01\n",
      "Episode 70 Min/Avg/Max 3.8300E+00/2.3737E+01/3.3770E+01\n",
      "Episode 71 Min/Avg/Max 1.8470E+01/2.5520E+01/3.6920E+01\n",
      "Episode 72 Min/Avg/Max 1.6890E+01/2.6146E+01/3.4080E+01\n",
      "Episode 73 Min/Avg/Max 1.7850E+01/2.2934E+01/3.0520E+01\n",
      "Episode 74 Min/Avg/Max 1.8000E+01/2.2890E+01/3.4070E+01\n",
      "Episode 75 Min/Avg/Max 1.7710E+01/2.5531E+01/3.1370E+01\n",
      "Episode 76 Min/Avg/Max 1.4960E+01/2.6997E+01/3.8470E+01\n",
      "Episode 77 Min/Avg/Max 1.8850E+01/2.6471E+01/3.9150E+01\n",
      "Episode 78 Min/Avg/Max 2.3320E+01/3.0673E+01/3.4810E+01\n",
      "Episode 79 Min/Avg/Max 2.5620E+01/3.1472E+01/3.6250E+01\n",
      "Episode 80 Min/Avg/Max 2.4210E+01/3.2033E+01/3.8540E+01\n",
      "Episode 81 Min/Avg/Max 2.8450E+01/3.4530E+01/3.8020E+01\n",
      "Episode 82 Min/Avg/Max 2.8080E+01/3.3722E+01/3.8040E+01\n",
      "Episode 83 Min/Avg/Max 2.8920E+01/3.4246E+01/3.7740E+01\n",
      "Episode 84 Min/Avg/Max 2.7680E+01/3.2669E+01/3.7130E+01\n",
      "Episode 85 Min/Avg/Max 2.8960E+01/3.6052E+01/3.8970E+01\n",
      "Episode 86 Min/Avg/Max 3.0360E+01/3.6226E+01/3.8700E+01\n",
      "Episode 87 Min/Avg/Max 3.3210E+01/3.5673E+01/3.8020E+01\n",
      "Episode 88 Min/Avg/Max 2.8350E+01/3.5271E+01/3.8300E+01\n",
      "Episode 89 Min/Avg/Max 3.1980E+01/3.5487E+01/3.9260E+01\n",
      "Episode 90 Min/Avg/Max 3.1730E+01/3.5195E+01/3.8300E+01\n",
      "Episode 91 Min/Avg/Max 3.2000E+01/3.5981E+01/3.8880E+01\n",
      "Episode 92 Min/Avg/Max 3.2580E+01/3.6531E+01/3.8920E+01\n",
      "Episode 93 Min/Avg/Max 3.2540E+01/3.6420E+01/3.8880E+01\n",
      "Episode 94 Min/Avg/Max 3.3470E+01/3.7637E+01/3.9140E+01\n",
      "Episode 95 Min/Avg/Max 3.1390E+01/3.6735E+01/3.9100E+01\n",
      "Episode 96 Min/Avg/Max 3.2940E+01/3.7247E+01/3.9170E+01\n",
      "Episode 97 Min/Avg/Max 3.2570E+01/3.6340E+01/3.8420E+01\n",
      "Episode 98 Min/Avg/Max 3.3710E+01/3.7676E+01/3.9120E+01\n",
      "Episode 99 Min/Avg/Max 3.1160E+01/3.6615E+01/3.9310E+01\n",
      "Episode 100 Min/Avg/Max 3.4260E+01/3.7524E+01/3.9190E+01\n",
      "Episode 101 Min/Avg/Max 3.1570E+01/3.6159E+01/3.8270E+01\n",
      "Episode 102 Min/Avg/Max 3.1230E+01/3.6967E+01/3.9400E+01\n",
      "Episode 103 Min/Avg/Max 3.3300E+01/3.6917E+01/3.8800E+01\n",
      "Episode 104 Min/Avg/Max 2.7340E+01/3.6451E+01/3.9500E+01\n",
      "Episode 105 Min/Avg/Max 3.3550E+01/3.8025E+01/3.9630E+01\n",
      "Episode 106 Min/Avg/Max 3.1880E+01/3.7195E+01/3.9410E+01\n",
      "Episode 107 Min/Avg/Max 2.7880E+01/3.5491E+01/3.8870E+01\n",
      "Episode 108 Min/Avg/Max 3.2640E+01/3.7637E+01/3.9350E+01\n",
      "Episode 109 Min/Avg/Max 2.7770E+01/3.5045E+01/3.8810E+01\n",
      "Episode 110 Min/Avg/Max 2.9560E+01/3.5740E+01/3.9650E+01\n",
      "Episode 111 Min/Avg/Max 3.3020E+01/3.7428E+01/3.9560E+01\n",
      "Episode 112 Min/Avg/Max 3.2640E+01/3.6410E+01/3.9030E+01\n",
      "Episode 113 Min/Avg/Max 3.2870E+01/3.7730E+01/3.9290E+01\n",
      "Episode 114 Min/Avg/Max 2.9160E+01/3.6666E+01/3.9520E+01\n",
      "Episode 115 Min/Avg/Max 3.5080E+01/3.7108E+01/3.8960E+01\n",
      "Episode 116 Min/Avg/Max 3.3340E+01/3.6376E+01/3.9230E+01\n",
      "Episode 117 Min/Avg/Max 3.2060E+01/3.5948E+01/3.9150E+01\n",
      "Episode 118 Min/Avg/Max 2.6710E+01/3.5910E+01/3.9450E+01\n",
      "Episode 119 Min/Avg/Max 3.3720E+01/3.7516E+01/3.9660E+01\n",
      "Episode 120 Min/Avg/Max 2.7620E+01/3.4974E+01/3.8130E+01\n",
      "Episode 121 Min/Avg/Max 2.7430E+01/3.4160E+01/3.7320E+01\n",
      "Episode 122 Min/Avg/Max 3.2620E+01/3.6229E+01/3.9240E+01\n",
      "Episode 123 Min/Avg/Max 3.4880E+01/3.6998E+01/3.9510E+01\n",
      "Episode 124 Min/Avg/Max 3.4540E+01/3.6757E+01/3.8360E+01\n",
      "Episode 125 Min/Avg/Max 3.3350E+01/3.6766E+01/3.9290E+01\n",
      "Episode 126 Min/Avg/Max 3.1580E+01/3.5870E+01/3.8880E+01\n",
      "Episode 127 Min/Avg/Max 3.3240E+01/3.6510E+01/3.9490E+01\n",
      "Episode 128 Min/Avg/Max 3.3190E+01/3.5692E+01/3.8690E+01\n",
      "Episode 129 Min/Avg/Max 3.3270E+01/3.5582E+01/3.8110E+01\n",
      "Episode 130 Min/Avg/Max 3.3920E+01/3.6585E+01/3.9500E+01\n",
      "Episode 131 Min/Avg/Max 3.6080E+01/3.7839E+01/3.9130E+01\n",
      "Episode 132 Min/Avg/Max 3.3580E+01/3.6899E+01/3.8710E+01\n",
      "Episode 133 Min/Avg/Max 3.2370E+01/3.6315E+01/3.9220E+01\n",
      "Episode 134 Min/Avg/Max 3.4050E+01/3.7215E+01/3.9230E+01\n",
      "Episode 135 Min/Avg/Max 2.8000E+01/3.6071E+01/3.9540E+01\n",
      "Episode 136 Min/Avg/Max 3.0570E+01/3.6055E+01/3.8630E+01\n",
      "Episode 137 Min/Avg/Max 3.4910E+01/3.6544E+01/3.8550E+01\n",
      "Episode 138 Min/Avg/Max 2.9500E+01/3.5552E+01/3.8820E+01\n",
      "Episode 139 Min/Avg/Max 2.8540E+01/3.4381E+01/3.7760E+01\n",
      "Episode 140 Min/Avg/Max 2.6390E+01/3.4577E+01/3.8720E+01\n",
      "Episode 141 Min/Avg/Max 2.1160E+01/3.1351E+01/3.8160E+01\n",
      "Episode 142 Min/Avg/Max 3.1230E+01/3.5645E+01/3.8650E+01\n",
      "Episode 143 Min/Avg/Max 2.6550E+01/3.2682E+01/3.6410E+01\n",
      "Episode 144 Min/Avg/Max 1.9110E+01/3.0698E+01/3.5710E+01\n",
      "Episode 145 Min/Avg/Max 2.6730E+01/3.4276E+01/3.7770E+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 146 Min/Avg/Max 2.5660E+01/3.3552E+01/3.7730E+01\n",
      "Episode 147 Min/Avg/Max 2.8650E+01/3.4613E+01/3.8540E+01\n",
      "Episode 148 Min/Avg/Max 3.3050E+01/3.5167E+01/3.7330E+01\n",
      "Episode 149 Min/Avg/Max 1.9800E+01/3.2705E+01/3.7280E+01\n",
      "Episode 150 Min/Avg/Max 2.8370E+01/3.5833E+01/3.8690E+01\n",
      "Episode 151 Min/Avg/Max 2.8380E+01/3.4414E+01/3.8100E+01\n",
      "Episode 152 Min/Avg/Max 2.5810E+01/3.3573E+01/3.7340E+01\n",
      "Episode 153 Min/Avg/Max 2.4940E+01/3.2297E+01/3.6100E+01\n",
      "Episode 154 Min/Avg/Max 3.1640E+01/3.5191E+01/3.7910E+01\n",
      "Episode 155 Min/Avg/Max 2.8340E+01/3.2633E+01/3.6710E+01\n",
      "Episode 156 Min/Avg/Max 2.8640E+01/3.3764E+01/3.7350E+01\n",
      "Episode 157 Min/Avg/Max 2.7990E+01/3.4350E+01/3.9550E+01\n",
      "Episode 158 Min/Avg/Max 2.6170E+01/3.3082E+01/3.8530E+01\n",
      "Episode 159 Min/Avg/Max 3.0450E+01/3.5137E+01/3.7640E+01\n",
      "Episode 160 Min/Avg/Max 3.1940E+01/3.5542E+01/3.7790E+01\n",
      "Episode 161 Min/Avg/Max 2.7070E+01/3.4119E+01/3.7370E+01\n",
      "Episode 162 Min/Avg/Max 2.7540E+01/3.5375E+01/3.7880E+01\n",
      "Episode 163 Min/Avg/Max 3.0170E+01/3.6848E+01/3.9310E+01\n",
      "Episode 164 Min/Avg/Max 2.0400E+01/3.3733E+01/3.8030E+01\n",
      "Episode 165 Min/Avg/Max 2.9520E+01/3.4000E+01/3.7340E+01\n",
      "Episode 166 Min/Avg/Max 3.1450E+01/3.6379E+01/3.8920E+01\n",
      "Episode 167 Min/Avg/Max 2.9560E+01/3.4512E+01/3.8080E+01\n",
      "Episode 168 Min/Avg/Max 3.0080E+01/3.5659E+01/3.9040E+01\n",
      "Episode 169 Min/Avg/Max 2.7140E+01/3.5567E+01/3.9280E+01\n",
      "Episode 170 Min/Avg/Max 3.1540E+01/3.4584E+01/3.6200E+01\n",
      "Episode 171 Min/Avg/Max 3.3220E+01/3.6607E+01/3.8470E+01\n",
      "Episode 172 Min/Avg/Max 3.3410E+01/3.6608E+01/3.8900E+01\n",
      "Episode 173 Min/Avg/Max 3.2470E+01/3.7532E+01/3.9490E+01\n",
      "Episode 174 Min/Avg/Max 3.0660E+01/3.5552E+01/3.8770E+01\n",
      "Episode 175 Min/Avg/Max 3.2640E+01/3.7019E+01/3.9080E+01\n",
      "Episode 176 Min/Avg/Max 2.6130E+01/3.4279E+01/3.9020E+01\n",
      "Episode 177 Min/Avg/Max 3.2300E+01/3.6443E+01/3.9210E+01\n",
      "Episode 178 Min/Avg/Max 3.4700E+01/3.7282E+01/3.9050E+01\n",
      "Episode 179 Min/Avg/Max 3.3670E+01/3.7194E+01/3.9290E+01\n",
      "Episode 180 Min/Avg/Max 3.1430E+01/3.4999E+01/3.7490E+01\n",
      "Episode 181 Min/Avg/Max 3.4630E+01/3.7109E+01/3.9550E+01\n",
      "Episode 182 Min/Avg/Max 3.1830E+01/3.5615E+01/3.8570E+01\n",
      "Episode 183 Min/Avg/Max 3.1550E+01/3.5611E+01/3.8130E+01\n",
      "Episode 184 Min/Avg/Max 2.9860E+01/3.3955E+01/3.7810E+01\n",
      "Episode 185 Min/Avg/Max 3.0770E+01/3.4597E+01/3.7750E+01\n",
      "Episode 186 Min/Avg/Max 2.9020E+01/3.4114E+01/3.6600E+01\n",
      "Episode 187 Min/Avg/Max 3.1700E+01/3.5041E+01/3.8080E+01\n",
      "Episode 188 Min/Avg/Max 3.2330E+01/3.6468E+01/3.8390E+01\n",
      "Episode 189 Min/Avg/Max 3.2250E+01/3.4846E+01/3.7240E+01\n",
      "Episode 190 Min/Avg/Max 3.0260E+01/3.5773E+01/3.9510E+01\n",
      "Episode 191 Min/Avg/Max 3.3550E+01/3.6316E+01/3.9230E+01\n",
      "Episode 192 Min/Avg/Max 3.1450E+01/3.4901E+01/3.6870E+01\n",
      "Episode 193 Min/Avg/Max 2.8650E+01/3.5125E+01/3.7510E+01\n",
      "Episode 194 Min/Avg/Max 3.0260E+01/3.4245E+01/3.6560E+01\n",
      "Episode 195 Min/Avg/Max 2.9560E+01/3.5913E+01/3.8610E+01\n",
      "Episode 196 Min/Avg/Max 2.9710E+01/3.6321E+01/3.8610E+01\n",
      "Episode 197 Min/Avg/Max 2.7260E+01/3.3766E+01/3.6580E+01\n",
      "Episode 198 Min/Avg/Max 2.6890E+01/3.2570E+01/3.6760E+01\n",
      "Episode 199 Min/Avg/Max 3.0650E+01/3.5839E+01/3.8140E+01\n",
      "Episode 200 Min/Avg/Max 2.9750E+01/3.3850E+01/3.7240E+01\n",
      "Episode 201 Min/Avg/Max 3.2880E+01/3.6943E+01/3.9540E+01\n",
      "Episode 202 Min/Avg/Max 3.3110E+01/3.5723E+01/3.7790E+01\n",
      "Episode 203 Min/Avg/Max 3.3220E+01/3.6056E+01/3.8720E+01\n",
      "Episode 204 Min/Avg/Max 2.6120E+01/3.3516E+01/3.7530E+01\n",
      "Episode 205 Min/Avg/Max 2.9120E+01/3.3515E+01/3.6760E+01\n",
      "Episode 206 Min/Avg/Max 2.9720E+01/3.4825E+01/3.7250E+01\n",
      "Episode 207 Min/Avg/Max 2.9720E+01/3.3745E+01/3.7560E+01\n",
      "Episode 208 Min/Avg/Max 3.0450E+01/3.4995E+01/3.8600E+01\n",
      "Episode 209 Min/Avg/Max 2.9010E+01/3.5101E+01/3.7560E+01\n",
      "Episode 210 Min/Avg/Max 3.1090E+01/3.6045E+01/3.8810E+01\n",
      "Episode 211 Min/Avg/Max 3.0110E+01/3.3636E+01/3.6640E+01\n",
      "Episode 212 Min/Avg/Max 2.6380E+01/3.1419E+01/3.4150E+01\n",
      "Episode 213 Min/Avg/Max 3.0080E+01/3.4462E+01/3.7550E+01\n",
      "Episode 214 Min/Avg/Max 2.9950E+01/3.4153E+01/3.6420E+01\n",
      "Episode 215 Min/Avg/Max 3.1830E+01/3.4586E+01/3.7420E+01\n",
      "Episode 216 Min/Avg/Max 2.8900E+01/3.3026E+01/3.5490E+01\n",
      "Episode 217 Min/Avg/Max 3.2160E+01/3.5757E+01/3.8250E+01\n",
      "Episode 218 Min/Avg/Max 3.2540E+01/3.6272E+01/3.8360E+01\n",
      "Episode 219 Min/Avg/Max 3.2760E+01/3.6425E+01/3.9030E+01\n",
      "Episode 220 Min/Avg/Max 3.2650E+01/3.6129E+01/3.8440E+01\n",
      "Episode 221 Min/Avg/Max 3.4810E+01/3.7083E+01/3.9150E+01\n",
      "Episode 222 Min/Avg/Max 3.0050E+01/3.4948E+01/3.8290E+01\n",
      "Episode 223 Min/Avg/Max 3.2290E+01/3.4869E+01/3.7510E+01\n",
      "Episode 224 Min/Avg/Max 3.5290E+01/3.6917E+01/3.8270E+01\n",
      "Episode 225 Min/Avg/Max 2.5560E+01/3.4644E+01/3.8080E+01\n",
      "Episode 226 Min/Avg/Max 3.2610E+01/3.5061E+01/3.7870E+01\n",
      "Episode 227 Min/Avg/Max 3.2110E+01/3.4685E+01/3.7840E+01\n",
      "Episode 228 Min/Avg/Max 2.9950E+01/3.5847E+01/3.8300E+01\n",
      "Episode 229 Min/Avg/Max 2.6700E+01/3.1821E+01/3.5550E+01\n",
      "Episode 230 Min/Avg/Max 3.1010E+01/3.3434E+01/3.7230E+01\n",
      "Episode 231 Min/Avg/Max 3.1060E+01/3.5125E+01/3.8040E+01\n",
      "Episode 232 Min/Avg/Max 3.0890E+01/3.4643E+01/3.7670E+01\n",
      "Episode 233 Min/Avg/Max 2.9310E+01/3.4649E+01/3.7400E+01\n",
      "Episode 234 Min/Avg/Max 3.4090E+01/3.6627E+01/3.8390E+01\n",
      "Episode 235 Min/Avg/Max 2.9520E+01/3.4365E+01/3.6720E+01\n",
      "Episode 236 Min/Avg/Max 2.2990E+01/3.2731E+01/3.6660E+01\n",
      "Episode 237 Min/Avg/Max 2.2770E+01/3.2329E+01/3.8300E+01\n",
      "Episode 238 Min/Avg/Max 2.1690E+01/3.0556E+01/3.7240E+01\n",
      "Episode 239 Min/Avg/Max 2.3730E+01/3.3490E+01/3.8200E+01\n",
      "Episode 240 Min/Avg/Max 1.9270E+01/3.3613E+01/3.8210E+01\n",
      "Episode 241 Min/Avg/Max 3.3020E+01/3.6220E+01/3.8540E+01\n",
      "Episode 242 Min/Avg/Max 2.4020E+01/3.4079E+01/3.8290E+01\n",
      "Episode 243 Min/Avg/Max 3.1010E+01/3.4312E+01/3.7200E+01\n",
      "Episode 244 Min/Avg/Max 3.6120E+01/3.7934E+01/3.9180E+01\n",
      "Episode 245 Min/Avg/Max 3.2000E+01/3.6236E+01/3.9010E+01\n",
      "Episode 246 Min/Avg/Max 2.9510E+01/3.4297E+01/3.8080E+01\n",
      "Episode 247 Min/Avg/Max 3.1270E+01/3.5035E+01/3.8470E+01\n",
      "Episode 248 Min/Avg/Max 2.9640E+01/3.5462E+01/3.8300E+01\n",
      "Episode 249 Min/Avg/Max 3.0740E+01/3.5714E+01/3.9540E+01\n",
      "Episode 250 Min/Avg/Max 3.4350E+01/3.6534E+01/3.8120E+01\n",
      "Episode 251 Min/Avg/Max 3.5240E+01/3.7269E+01/3.9040E+01\n",
      "Episode 252 Min/Avg/Max 3.3440E+01/3.5932E+01/3.7290E+01\n",
      "Episode 253 Min/Avg/Max 3.0540E+01/3.6208E+01/3.8540E+01\n",
      "Episode 254 Min/Avg/Max 2.6590E+01/3.4526E+01/3.7240E+01\n",
      "Episode 255 Min/Avg/Max 2.9730E+01/3.5149E+01/3.7990E+01\n",
      "Episode 256 Min/Avg/Max 3.3610E+01/3.6298E+01/3.8650E+01\n",
      "Episode 257 Min/Avg/Max 3.2470E+01/3.7086E+01/3.8550E+01\n",
      "Episode 258 Min/Avg/Max 3.3360E+01/3.6242E+01/3.8050E+01\n",
      "Episode 259 Min/Avg/Max 3.2630E+01/3.5307E+01/3.8710E+01\n",
      "Episode 260 Min/Avg/Max 3.2340E+01/3.5473E+01/3.8060E+01\n",
      "Episode 261 Min/Avg/Max 3.2030E+01/3.5481E+01/3.9080E+01\n",
      "Episode 262 Min/Avg/Max 2.9930E+01/3.4494E+01/3.7410E+01\n",
      "Episode 263 Min/Avg/Max 3.4070E+01/3.6551E+01/3.8600E+01\n",
      "Episode 264 Min/Avg/Max 3.2140E+01/3.5445E+01/3.7980E+01\n",
      "Episode 265 Min/Avg/Max 3.1660E+01/3.5980E+01/3.8600E+01\n",
      "Episode 266 Min/Avg/Max 3.0030E+01/3.5333E+01/3.8150E+01\n",
      "Episode 267 Min/Avg/Max 2.6630E+01/3.2625E+01/3.6900E+01\n",
      "Episode 268 Min/Avg/Max 3.0580E+01/3.5793E+01/3.8580E+01\n",
      "Episode 269 Min/Avg/Max 3.2200E+01/3.4755E+01/3.7690E+01\n",
      "Episode 270 Min/Avg/Max 3.0780E+01/3.4396E+01/3.6460E+01\n",
      "Episode 271 Min/Avg/Max 3.3330E+01/3.5534E+01/3.8630E+01\n",
      "Episode 272 Min/Avg/Max 3.2780E+01/3.6012E+01/3.7820E+01\n",
      "Episode 273 Min/Avg/Max 3.1970E+01/3.5641E+01/3.7890E+01\n",
      "Episode 274 Min/Avg/Max 3.0820E+01/3.5127E+01/3.7380E+01\n",
      "Episode 275 Min/Avg/Max 3.1890E+01/3.4813E+01/3.7910E+01\n",
      "Episode 276 Min/Avg/Max 2.5190E+01/3.0647E+01/3.5390E+01\n",
      "Episode 277 Min/Avg/Max 2.7530E+01/3.2124E+01/3.6720E+01\n",
      "Episode 278 Min/Avg/Max 3.2820E+01/3.6348E+01/3.8800E+01\n",
      "Episode 279 Min/Avg/Max 3.2200E+01/3.4755E+01/3.7420E+01\n",
      "Episode 280 Min/Avg/Max 3.3910E+01/3.5902E+01/3.8270E+01\n",
      "Episode 281 Min/Avg/Max 3.4160E+01/3.6550E+01/3.8630E+01\n",
      "Episode 282 Min/Avg/Max 3.3540E+01/3.6173E+01/3.8430E+01\n",
      "Episode 283 Min/Avg/Max 3.3620E+01/3.6721E+01/3.8880E+01\n",
      "Episode 284 Min/Avg/Max 3.1340E+01/3.5031E+01/3.8310E+01\n",
      "Episode 285 Min/Avg/Max 3.2980E+01/3.5396E+01/3.7820E+01\n",
      "Episode 286 Min/Avg/Max 3.4980E+01/3.6792E+01/3.7930E+01\n",
      "Episode 287 Min/Avg/Max 3.1830E+01/3.5294E+01/3.8200E+01\n",
      "Episode 288 Min/Avg/Max 3.1890E+01/3.5844E+01/3.8330E+01\n",
      "Episode 289 Min/Avg/Max 3.2980E+01/3.6386E+01/3.8820E+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 290 Min/Avg/Max 2.8860E+01/3.2479E+01/3.5510E+01\n",
      "Episode 291 Min/Avg/Max 3.1250E+01/3.4989E+01/3.8100E+01\n",
      "Episode 292 Min/Avg/Max 3.3850E+01/3.5908E+01/3.8340E+01\n",
      "Episode 293 Min/Avg/Max 3.3410E+01/3.5801E+01/3.7300E+01\n",
      "Episode 294 Min/Avg/Max 2.8820E+01/3.4263E+01/3.8160E+01\n",
      "Episode 295 Min/Avg/Max 2.8240E+01/3.4437E+01/3.6860E+01\n",
      "Episode 296 Min/Avg/Max 3.2400E+01/3.4784E+01/3.6840E+01\n",
      "Episode 297 Min/Avg/Max 3.3890E+01/3.6343E+01/3.8550E+01\n",
      "Episode 298 Min/Avg/Max 3.2360E+01/3.5609E+01/3.7370E+01\n",
      "Episode 299 Min/Avg/Max 3.6390E+01/3.8057E+01/3.9460E+01\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "buffer_size = 1000000\n",
    "use_batch_norm = True\n",
    "fn_name = 'normal'\n",
    "fname_suffix = '{}_{}_{}_{}'.format(batch_size, buffer_size, use_batch_norm, fn_name)\n",
    "\n",
    "scores, agent = train_ddpg(batch_size,buffer_size,use_batch_norm,np.random.randn, 300)\n",
    "end_time = time.time()\n",
    "fname = './data/time_and_scores_{}_official.pkl'.format(fname_suffix)\n",
    "\n",
    "pickle.dump((end_time-start_time, scores), open(fname, 'wb'))\n",
    "torch.save(agent.actor_local.state_dict(), './data/actor_{}_official.pt'.format(fname_suffix))\n",
    "torch.save(agent.critic_local.state_dict(), './data/critic_{}_official.pt'.format(fname_suffix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-Trained Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "0) Average Score: 37.35449916506186\n",
      "1) Average Score: 38.07999914884567\n",
      "2) Average Score: 37.03899917211383\n",
      "3) Average Score: 37.216999168135224\n",
      "4) Average Score: 38.07599914893508\n"
     ]
    }
   ],
   "source": [
    "# Load saved models into memory\n",
    "from ddpg_agent import Agent\n",
    "import torch\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# close an instance if it exists\n",
    "try: env\n",
    "except NameError: env = None\n",
    "if env:\n",
    "    env.close()\n",
    "env = UnityEnvironment(file_name='./Reacher_Linux/Reacher.x86_64')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "numAgents = 20\n",
    "agent = Agent(numAgents = numAgents, \n",
    "                  state_size=state_size, \n",
    "                  action_size=action_size, \n",
    "                  random_seed=47,\n",
    "                  use_batch_norm = True,\n",
    "                  random_fn = np.random.randn)\n",
    "agent.actor_local.load_state_dict(torch.load(\"./data/actor_128_1000000_True_normal_official.pt\"))\n",
    "\n",
    "numEpisodes = 5\n",
    "\n",
    "mean_scores = []\n",
    "for i in range(numEpisodes):\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    states = env_info.vector_observations            # get the current state\n",
    "    scores = np.zeros(numAgents)\n",
    "    while True:\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        \n",
    "        if np.any(dones):\n",
    "            break\n",
    "    print(\"{}) Average Score: {}\".format(i, np.mean(scores)))\n",
    "    mean_scores.append(np.mean(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fname = './data/trained_model_scores.pkl'\n",
    "pickle.dump(mean_scores, open(fname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
